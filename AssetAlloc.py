# -*- coding: utf-8 -*-
"""Rubrics MVO FINAL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EC20xO8KdZ46IxVfFvhk2MZtAaXZ7ZDC

#**Mean-Variance Optimisation (MVO) Framework**

This code implements a Mean-Variance Optimisation (MVO) framework for fixed income portfolios using **historic returns** as a proxy for expected returns. It is specifically tailored for analysing three funds (GFI, GCF, EYF), each with unique constraints reflecting investment policy and risk appetite. The process involves loading raw data, preparing it for modelling, applying portfolio optimisation with constraints, and visualising and interpreting the results.

###Import Packages
"""

import pandas as pd
import numpy as np
import cvxpy as cp
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.covariance import LedoitWolf

"""###Load File"""

from google.colab import files
uploaded = files.upload()

# Extract uploaded file name
file_name = list(uploaded.keys())[0]

# Load Excel file
xls = pd.ExcelFile(file_name)

# Load both sheets
df_raw = xls.parse('Index List')       # Sheet 1: Price histories
df_metadata_raw = xls.parse('Sheet2')  # Sheet 2: Metadata

"""#Exploratory Data Analysis and Preprocessing"""

df_raw.head()

df_metadata_raw

# Identify date and value columns
date_columns = df_raw.columns[::2]
value_columns = df_raw.columns[1::2]

# Get the intersection of all dates across each date column
date_sets = []
for col in date_columns:
    dates = pd.to_datetime(df_raw[col].dropna().unique())
    date_sets.append(set(dates))
common_dates = sorted(set.intersection(*date_sets))

# Clean each (Date, Value) pair and drop duplicate dates
series_list = []
for date_col, value_col in zip(date_columns, value_columns):
    temp_df = df_raw[[date_col, value_col]].copy()
    temp_df.columns = ['Date', value_col]
    temp_df['Date'] = pd.to_datetime(temp_df['Date'])
    temp_df = temp_df[temp_df['Date'].isin(common_dates)]
    temp_df = temp_df.drop_duplicates(subset='Date')
    series_list.append(temp_df)

# Merge all series on 'Date' using a loop
df_common = series_list[0]
for i in range(1, len(series_list)):
    df_common = pd.merge(df_common, series_list[i], on='Date', how='inner')

# Set 'Date' as index
df_common.set_index('Date', inplace=True)

# Create an empty dictionary to store the results
first_last_dates = {}

# Iterate over each column (variable) in the DataFrame
for col in df_common.columns:
    # Find the first and last non-missing dates for the current variable
    first_date = df_common[col].first_valid_index()
    last_date = df_common[col].last_valid_index()

    # Store the results in the dictionary
    first_last_dates[col] = {'first_date': first_date, 'last_date': last_date}

# Convert the dictionary to a DataFrame for better visualization
result_df = pd.DataFrame.from_dict(first_last_dates, orient='index')

# Display the resulting DataFrame
result_df

df_common.head()

df_common.dtypes

import plotly.graph_objects as go

fig = go.Figure()

for column in df_common.columns:
    fig.add_trace(
        go.Scatter(x=df_common.index, y=df_common[column], mode='lines', name=column)
    )

fig.update_layout(title='Interactive Plot of Indices',
                  xaxis_title='Date',
                  yaxis_title='Index Value')

fig.show()

df_common.describe()

# Calculate percentage change for each index and drop the first row.
df_pct_change = df_common.pct_change()
df_pct_change.dropna(inplace=True)
df_pct_change.head()

import plotly.graph_objects as go

fig = go.Figure()

for col in df_pct_change.columns:
    fig.add_trace(go.Scatter(x=df_pct_change.index, y=df_pct_change[col],
                             mode='lines', name=col, hovertemplate = '%{y:.2f}'))

fig.update_layout(title="Percentage Change of Index Values Over Time",
                  xaxis_title="Date",
                  yaxis_title="Percentage Change",
                  hovermode="closest")

fig.show()

# Rebase all indices to 100
df_rebased = df_common / df_common.iloc[0] * 100

fig = go.Figure()

for col in df_rebased.columns:
    fig.add_trace(go.Scatter(x=df_rebased.index, y=df_rebased[col],
                             mode='lines', name=col, hovertemplate = '%{y:.2f}'))

fig.update_layout(title="Rebased Index Values Over Time",
                  xaxis_title="Date",
                  yaxis_title="Index Value (Rebased to 100)",
                  hovermode="closest")

fig.show()

# Calculate annualized returns and standard deviations
annualized_returns = df_pct_change.mean() * 252 * 100  # Assuming 252 trading days per year
annualized_std = df_pct_change.std() * np.sqrt(252) * 100

# Create a DataFrame for results
results_df = pd.DataFrame({'Annualised Return (%)': annualized_returns, 'Standard Deviation (%)': annualized_std})

# Rank by annualized return in descending order
results_df = results_df.sort_values(by='Annualised Return (%)', ascending=False)

# Display the table
results_df

# Select Top 12 assets by Annualised Return
top_assets = results_df.head(12).index

# Calculate Correlation Matrix and round
correlation_matrix_summary = df_pct_change[top_assets].corr().round(2)

# Save for later use in the LLM query
correlation_matrix_text = correlation_matrix_summary.to_string()

# Assuming df_pct_change is the DataFrame with percentage change data
correlation_matrix = df_pct_change.corr()

# Optional: Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Percentage Change')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

n_cols = 3  # Adjust columns for layout
n_assets = len(df_pct_change.columns)
n_rows = int(np.ceil(n_assets / n_cols))

plt.figure(figsize=(n_cols * 5, n_rows * 3.5))
for i, col in enumerate(df_pct_change.columns):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(df_pct_change[col], bins=50, kde=True, color='steelblue')
    plt.title(f'{col} Distribution')
    plt.xlabel('Daily % Change')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.suptitle("Histogram & KDE of Daily Returns", fontsize=16, y=1.02)
plt.show()

summary_stats = pd.DataFrame({
    'Skewness': df_pct_change.skew(),
    'Kurtosis': df_pct_change.kurtosis()
})
display(summary_stats.sort_values(by='Skewness', key=abs, ascending=False))

"""###Align Metadata with Returns"""

# Ensure aligned assets & order between df_pct_change and df_metadata
available_names = df_pct_change.columns.intersection(df_metadata_raw['Name'])

df_pct_change = df_pct_change[available_names]
df_metadata = df_metadata_raw[df_metadata_raw['Name'].isin(available_names)].set_index('Name')

# Enforce same order
df_metadata = df_metadata.loc[df_pct_change.columns]

# Confirm
assert all(df_pct_change.columns == df_metadata.index), "Mismatch in index order"

# Define historical and current risk-free rates
rf_rate_hist = df_pct_change['US T-Bills'].mean() * 252
rf_rate_current = df_metadata.loc['US T-Bills', 'Current Yield Hdgd'] / 100

print(f"Historical Risk-Free Rate (T-Bills avg return): {rf_rate_hist:.4%}")
print(f"Current T-Bill Yield (from metadata): {rf_rate_current:.4%}")

"""#Enter Fund Constraints"""

# Define rating scale (numeric mapping)
rating_scale = {
    'AAA': 20, 'AA+': 19, 'AA': 18, 'AA-': 17,
    'A+': 16, 'A': 15, 'A-': 14,
    'BBB+': 13, 'BBB': 12, 'BBB-': 11,
    'BB+': 10, 'BB': 9, 'BB-': 8,
    'B+': 7, 'B': 6, 'B-': 5,
    'CCC+': 4, 'CCC': 3, 'CCC-': 2,
    'NR': 1
}

# Reverse lookup for display
inverse_rating_scale = {v: k for k, v in rating_scale.items()}

# Fund-Specific Constraints
fund_constraints = {
    'GFI': {
        'max_non_ig': 0.25,
        'max_em': 0.30,
        'max_at1': 0.15,
        'max_duration': 6.5,
        'min_rating': rating_scale['BBB-'],
        'max_hybrid': 0.15,
        'max_tbill': 0.20
    },
    'GCF': {
        'max_non_ig': 0.10,
        'max_em': 0.35,
        'max_at1': 0.10,
        'max_duration': 3.5,
        'min_rating': rating_scale['BBB'],
        'max_hybrid': 0.10,
        'max_tbill': 0.20
    },
    'EYF': {
        'max_non_ig': 1.0,
        'max_em': 1.0,
        'max_at1': 0.0001,
        'max_duration': None,
        'min_rating': rating_scale['BB'],
        'max_hybrid': 0.20,
        'max_tbill': 0.20
    }
}

"""#Optimiser Function"""

# Final alignment of returns and metadata
df_pct_change_final = df_pct_change.copy()
df_metadata_final = df_metadata.copy()

print("Assets in universe:", df_pct_change_final.columns.tolist())

df_metadata_final = df_metadata_final.loc[df_pct_change_final.columns]

# Force float types on binary flags
df_metadata_final[['Is_AT1', 'Is_EM', 'Is_Non_IG','Is_Hybrid']] = df_metadata_final[['Is_AT1', 'Is_EM', 'Is_Non_IG','Is_Hybrid']].astype(float)
df_metadata_final.loc['US T-Bills', ['Is_AT1', 'Is_EM', 'Is_Non_IG','Is_Hybrid']] = 0

def optimise_portfolio(fund_name, returns, metadata, constraints, rf, target_return):
    """
    Historical return-based optimiser:
    Minimise variance subject to target return & fund constraints.
    Now includes optional T-Bill exposure constraint.
    """
    import cvxpy as cp
    from sklearn.covariance import LedoitWolf
    import pandas as pd
    import numpy as np

    mu = returns.mean().values * 252  # Annualised historical returns
    cov = LedoitWolf().fit(returns).covariance_ * 252

    idx = returns.columns.tolist()
    n = len(idx)
    w = cp.Variable(n)

    metadata = metadata.loc[idx]
    rating = metadata['Rating_Num'].values
    duration = metadata['Duration'].values
    is_at1 = metadata['Is_AT1'].values.astype(float)
    is_em = metadata['Is_EM'].values.astype(float)
    is_non_ig = metadata['Is_Non_IG'].values.astype(float)
    is_hybrid = metadata['Is_Hybrid'].values.astype(float)
    yields = metadata['Current Yield Hdgd'].values / 100  # Used for reporting

    constraints_list = [
        cp.sum(w) == 1,
        w >= 0,
        is_non_ig @ w <= constraints['max_non_ig'],
        is_em @ w <= constraints['max_em'],
        is_at1 @ w <= constraints['max_at1'],
        is_hybrid @ w <= constraints['max_hybrid'],
        rating @ w >= constraints['min_rating']
    ]

    if constraints.get('max_tbill') is not None:
        tbill_index = idx.index('US T-Bills')
        constraints_list.append(w[tbill_index] <= constraints['max_tbill'])

    if constraints['max_duration'] is not None:
        constraints_list.append(duration @ w <= constraints['max_duration'])

    constraints_list.append(mu @ w >= target_return)

    problem = cp.Problem(cp.Minimize(cp.quad_form(w, cov)), constraints_list)
    problem.solve()

    if w.value is None:
        raise ValueError("No solution found")

    weights = pd.Series(w.value, index=idx).round(4)
    weights = weights[weights > 0.001].sort_values(ascending=False)

    # Post-solve constraint verification
    rating_avg = (rating @ w.value).item()
    if rating_avg < constraints['min_rating']:
        print(f"\n\u2757\ufe0f Constraint violation in {fund_name} @ target {target_return:.2%}:")
        print(f" - Rating constraint violated: {rating_avg:.2f} < {constraints['min_rating']}")
        raise ValueError("Rating constraint violated post-solve")

    tbill_weight = w.value[tbill_index].item() if 'max_tbill' in constraints else 0.0

    metrics = {
        'Expected Return': (mu @ w.value).item(),
        'Expected Volatility': np.sqrt((w.value).T @ cov @ w.value).item(),
        'Avg Yield': (yields @ w.value).item(),
        'Avg Duration': (duration @ w.value).item(),
        'Avg Rating': rating_avg,
        'EM Exposure': (is_em @ w.value).item(),
        'AT1 Exposure': (is_at1 @ w.value).item(),
        'Non-IG Exposure': (is_non_ig @ w.value).item(),
        'Hybrid Exposure': (is_hybrid @ w.value).item(),
        'T-Bill Exposure': tbill_weight
    }

    return weights, metrics

def find_max_return_only(fund_name, df_returns, df_metadata, fund_constraints):
    """
    Optimises for maximum achievable return, ignoring volatility.
    Used to define a true upper bound for the efficient frontier.
    """
    import cvxpy as cp

    mu = df_returns.mean() * 252
    idx = df_returns.columns.tolist()
    n = len(idx)
    w = cp.Variable(n)

    metadata = df_metadata.loc[idx]
    rating = metadata['Rating_Num'].values
    duration = metadata['Duration'].values
    is_at1 = metadata['Is_AT1'].values.astype(float)
    is_em = metadata['Is_EM'].values.astype(float)
    is_non_ig = metadata['Is_Non_IG'].values.astype(float)
    is_hybrid = metadata['Is_Hybrid'].values.astype(float)

    constraints_list = [
        cp.sum(w) == 1,
        w >= 0,
        is_non_ig @ w <= fund_constraints['max_non_ig'],
        is_em @ w <= fund_constraints['max_em'],
        is_at1 @ w <= fund_constraints['max_at1'],
        is_hybrid @ w <= fund_constraints['max_hybrid'],
        rating @ w >= fund_constraints['min_rating']
    ]

    if fund_constraints['max_duration'] is not None:
        constraints_list.append(duration @ w <= fund_constraints['max_duration'])

    # ✅ Add T-Bill constraint if present
    if 'max_tbill' in fund_constraints:
        tbill_index = idx.index('US T-Bills')
        constraints_list.append(w[tbill_index] <= fund_constraints['max_tbill'])

    problem = cp.Problem(cp.Maximize(mu.values @ w), constraints_list)
    problem.solve()

    if w.value is None:
        raise ValueError("No solution found when maximising return.")

    return (mu.values @ w.value).item()

def generate_efficient_frontier(fund_name, df_returns, df_metadata, fund_constraints, rf_rate_hist, step_size=0.0015):
    """
    Generates efficient frontier using historical returns as expected return,
    with fixed step size between targets for consistent portfolio spacing.
    Includes support for T-Bill allocation constraint.
    """
    print(f"--- Generating Efficient Frontier for: {fund_name} (Historical Returns with Fixed Step Size) ---")

    # Step 1: Minimum Return Portfolio (variance-minimised)
    try:
        w_min, m_min = optimise_portfolio(
            fund_name, df_returns, df_metadata, fund_constraints, rf_rate_hist,
            target_return = df_returns.mean().min() * 252 * 0.5
        )
    except Exception as e:
        raise ValueError(f"Cannot even build min return portfolio for {fund_name}: {e}")

    min_return = m_min['Expected Return']

    # Step 2: Maximum Return via return-only maximiser
    try:
        max_return_discovered = find_max_return_only(fund_name, df_returns, df_metadata, fund_constraints)
        max_return = max(max_return_discovered, min_return * 2)
    except:
        max_return = min_return * 2

    print(f"Min Return Found: {min_return:.4%}")
    print(f"Max Achievable Return Used: {max_return:.4%}")

    # Step 3: Create target return range with fixed step size
    targets = np.arange(min_return, max_return + step_size, step_size)

    # Step 4: Build frontier
    returns_list, risks_list, metrics_dict, weights_dict = [], [], {}, {}

    for i, target in enumerate(targets):
        try:
            w, m = optimise_portfolio(
                fund_name, df_returns, df_metadata, fund_constraints, rf_rate_hist, target
            )
            returns_list.append(m['Expected Return'])
            risks_list.append(m['Expected Volatility'])
            label = f"Portfolio {len(metrics_dict)+1}"
            metrics_dict[label] = m
            weights_dict[label] = w
        except Exception:
            continue  # skip infeasible targets

    if not metrics_dict:
        raise ValueError("No feasible portfolios found for the efficient frontier.")

    # Step 5: Build Output DataFrames
    df_metrics = pd.DataFrame(metrics_dict)
    df_weights = pd.concat(weights_dict.values(), axis=1)
    df_weights.columns = metrics_dict.keys()
    df_weights = df_weights.fillna(0).round(4)

    # Step 6: Sharpe Ratio (Hist RF)
    sharpe_hist = ((df_metrics.loc['Expected Return'] - rf_rate_hist) / df_metrics.loc['Expected Volatility']).round(2)
    df_metrics.loc['Sharpe (Hist Avg)'] = sharpe_hist

    print(f"Feasible Portfolios Found: {len(df_metrics.columns)}")

    return returns_list, risks_list, df_metrics, df_weights

def process_fund(fund_name, df_pct_change_final, df_metadata_final, fund_constraints, rf_rate_hist):
    returns_list, risks_list, df_metrics, df_weights = generate_efficient_frontier(
      fund_name, df_pct_change_final, df_metadata_final, fund_constraints[fund_name], rf_rate_hist
    )

    return {
        'Efficient Frontier Returns': returns_list,
        'Efficient Frontier Risks': risks_list,
        'Metrics Table': df_metrics,
        'Weights Table': df_weights
    }

results = {}
funds = ['GFI', 'GCF', 'EYF']

for fund in funds:
    print(f"\n{'-'*40}\nProcessing {fund}\n{'-'*40}")
    results[fund] = process_fund(fund, df_pct_change_final, df_metadata_final, fund_constraints, rf_rate_hist)

print("All funds processed. Results stored.")

# Plot Efficient Frontier (no optimal portfolio)
def plot_frontier(risks, returns, fund_name):
    metrics_table = results[fund_name]['Metrics Table']

    hover_texts = []
    for i, portfolio in enumerate(metrics_table.columns):
        m = metrics_table[portfolio]
        hover_texts.append(
            f"<b>{portfolio}</b><br>"
            f"Expected Return: {m['Expected Return']*100:.2f}%<br>"
            f"Volatility: {m['Expected Volatility']*100:.2f}%<br>"
            f"Sharpe (Hist Avg): {m['Sharpe (Hist Avg)']:.2f}<br>"
            f"Avg Yield: {m['Avg Yield']*100:.2f}%<br>"
            f"Avg Duration: {m['Avg Duration']:.2f} yrs<br>"
            f"Avg Rating: {inverse_rating_scale.get(int(round(m['Avg Rating'])), m['Avg Rating']):<}<br>"
            f"EM Exposure: {m['EM Exposure']*100:.2f}%<br>"
            f"AT1 Exposure: {m['AT1 Exposure']*100:.2f}%<br>"
            f"Non-IG Exposure: {m['Non-IG Exposure']*100:.2f}%<br>"
            f"Hybrid Exposure: {m['Hybrid Exposure']*100:.2f}%"
        )

    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=risks,
        y=returns,
        mode='lines+markers',
        name='Efficient Frontier',
        text=hover_texts,
        hoverinfo='text'
    ))

    fig.update_layout(
        title=f"{fund_name} Efficient Frontier",
        xaxis_title="Volatility (Standard Deviation)",
        yaxis_title="Expected Return",
        template="plotly_dark"
    )

    fig.show()

import plotly.graph_objects as go

def plot_weights_area_chart(df_weights, fund_name):
    df_pct = df_weights * 100
    df_pct = df_pct.div(df_pct.sum(axis=0), axis=1) * 100  # Normalise to 100%

    fig = go.Figure()

    for asset in df_pct.index:
        fig.add_trace(go.Scatter(
            x=df_pct.columns,
            y=df_pct.loc[asset],
            mode='lines+markers',
            stackgroup='one',
            name=asset,
            hovertemplate="<b>Asset:</b> %{fullData.name}<br><b>Portfolio:</b> %{x}<br><b>Weight:</b> %{y:.2f}%"  # Clean hover
        ))

    fig.update_layout(
        title=f"{fund_name} Portfolio Composition Across Frontier",
        xaxis_title="Portfolios Along Efficient Frontier",
        yaxis_title="Weight (%)",
        yaxis=dict(range=[0, 100], gridcolor='rgba(0,0,0,0.2)'),
        xaxis=dict(gridcolor='rgba(0,0,0,0.2)'),
        template="plotly_dark",  # Options: "plotly", "plotly_white", "plotly_dark", "ggplot2", "seaborn", "simple_white", "none"
        hovermode="closest",
        font=dict(size=12),
        legend=dict(title="Assets")
    )

    fig.show()

def format_metrics_table(df, fund_constraints=None, fund_name=None):
    header_row = '🔒 Constraint Budget Usage'
    percent_cols = ['Expected Return', 'Expected Volatility', 'EM Exposure', 'AT1 Exposure',
                    'Non-IG Exposure', 'Hybrid Exposure', 'T-Bill Exposure', 'Avg Yield']

    df_formatted = df.copy()
    df_display = df.copy().astype('object')

    for row in df_display.index:
        if row in percent_cols:
            df_display.loc[row] = (df_display.loc[row] * 100).round(2).map('{:.2f}%'.format).str.replace('-0.00%', '0.00%')
        elif row == 'Avg Rating':
            df_display.loc[row] = df_display.loc[row].apply(
                lambda x: inverse_rating_scale.get(int(round(x)), f"{x:.2f}")
            )
        else:
            df_display.loc[row] = df_display.loc[row].map('{:.4f}'.format)

    usage_labels = []

    if fund_constraints and fund_name:
        df_display.loc[''] = [''] * df_display.shape[1]
        df_formatted.loc[''] = [0] * df_formatted.shape[1]

        df_display.loc[header_row] = [''] * df_display.shape[1]
        df_formatted.loc[header_row] = [0] * df_formatted.shape[1]

        c = fund_constraints[fund_name]

        def clean_ratio(numerator, denom):
            if denom < 0.01:
                return (numerator > 1e-4).astype(float)  # Treat exposures > 0.01% as 100% usage
            return (numerator / denom).clip(0, 1).fillna(0)

        usage_rows = {
            f"AT1 (≤{int(c['max_at1'] * 100)}%)": clean_ratio(df.loc['AT1 Exposure'], max(c['max_at1'], 0.001)),
            f"EM (≤{int(c['max_em'] * 100)}%)": clean_ratio(df.loc['EM Exposure'], c['max_em']),
            f"Non-IG (≤{int(c['max_non_ig'] * 100)}%)": clean_ratio(df.loc['Non-IG Exposure'], c['max_non_ig']),
            f"Hybrid (≤{int(c['max_hybrid'] * 100)}%)": clean_ratio(df.loc['Hybrid Exposure'], c['max_hybrid']),
        }

        if c.get('max_tbill') is not None:
            usage_rows[f"T-Bills (≤{int(c['max_tbill'] * 100)}%)"] = clean_ratio(df.loc['T-Bill Exposure'], c['max_tbill'])

        if c['max_duration'] is not None:
            usage_rows[f"Duration (≤{c['max_duration']:.1f} yrs)"] = clean_ratio(df.loc['Avg Duration'], c['max_duration'])

        rating_range = 20 - c['min_rating']
        rating_usage = ((20 - df.loc['Avg Rating']) / rating_range).clip(0, 1)
        usage_rows[f"Rating (≥{inverse_rating_scale[c['min_rating']]})"] = rating_usage

        for row, series in usage_rows.items():
            df_display.loc[row] = (series * 100).round(2).map('{:.2f}%'.format).str.replace('-0.00%', '0.00%')
            df_formatted.loc[row] = series
            usage_labels.append(row)

    # 🎨 Constraint styling for dark mode
    def style_budget(val):
        if isinstance(val, str) and '%' in val:
            try:
                pct = float(val.replace('%', '')) / 100
                if pct == 0:
                    return ''
                elif pct < 0.4:
                    return 'background-color: #1a3c1a; color: white'
                elif pct < 0.7:
                    return 'background-color: #4d4d00; color: white'
                elif pct < 0.9:
                    return 'background-color: #804000; color: white'
                else:
                    return 'background-color: #660000; color: white'
            except:
                return ''
        return ''

    def style_header(val):
        return 'font-weight: bold; background-color: #444; color: white' if val == '' else ''

    styler = df_display.style

    if usage_labels:
        styler = styler.map(style_budget, subset=pd.IndexSlice[usage_labels, :])

    if header_row in df_display.index:
        styler = styler.applymap(style_header, subset=pd.IndexSlice[[header_row], :])

    return styler

def find_optimal_portfolio(fund_name):
    """
    Finds and displays the portfolio(s) with the highest Sharpe Ratio (Hist Avg) for a given fund.
    """
    df_metrics = results[fund_name]['Metrics Table']

    sharpe_series = df_metrics.loc['Sharpe (Hist Avg)'].astype(float)
    max_sharpe_value = sharpe_series.max()
    optimal_portfolios = sharpe_series[sharpe_series == max_sharpe_value].index.tolist()

    print(f"Optimal Portfolio(s) for {fund_name}:")
    print(f"Recommended Portfolio(s) → {', '.join(optimal_portfolios)}")
    print(f"Sharpe Ratio (Hist Avg) → {max_sharpe_value}")

"""#Results

##Rubrics Global Fixed Income UCITS Fund
"""

fund_to_plot = 'GFI'
plot_frontier(results[fund_to_plot]['Efficient Frontier Risks'], results[fund_to_plot]['Efficient Frontier Returns'], fund_to_plot)

results['GFI']['Weights Table'].style.format("{:.2%}")

# Call for GFI
df_weights = results['GFI']['Weights Table'].copy()
plot_weights_area_chart(df_weights, 'GFI')

format_metrics_table(results['GFI']['Metrics Table'], fund_constraints, 'GFI')

#Optimal Portfolio
find_optimal_portfolio('GFI')

"""##Rubrics Global Credit UCITS Fund"""

fund_to_plot = 'GCF'
plot_frontier(results[fund_to_plot]['Efficient Frontier Risks'], results[fund_to_plot]['Efficient Frontier Returns'], fund_to_plot)

results['GCF']['Weights Table'].style.format("{:.2%}")

# Call for GFI
df_weights = results['GCF']['Weights Table'].copy()
plot_weights_area_chart(df_weights, 'GCF')

format_metrics_table(results['GCF']['Metrics Table'], fund_constraints, 'GCF')

#Optimal Portfolio
find_optimal_portfolio('GCF')

"""##Rubrics Enhanced Yield UCITS Fund"""

fund_to_plot = 'EYF'
plot_frontier(results[fund_to_plot]['Efficient Frontier Risks'], results[fund_to_plot]['Efficient Frontier Returns'], fund_to_plot)

results['EYF']['Weights Table'].style.format("{:.2%}")

# Call for GFI
df_weights = results['EYF']['Weights Table'].copy()
plot_weights_area_chart(df_weights, 'EYF')

format_metrics_table(results['EYF']['Metrics Table'], fund_constraints, 'EYF')

#Optimal Portfolio
find_optimal_portfolio('EYF')

"""Sharpe Summary"""

# Identify Optimal Portfolio (Highest Sharpe) for each fund
optimal_portfolios = {}

for fund in ['GFI', 'GCF', 'EYF']:
    metrics_table = results[fund]["Metrics Table"]
    weights_table = results[fund]["Weights Table"]

    # Find Portfolio with Highest Sharpe (Hist Avg)
    optimal_label = metrics_table.loc['Sharpe (Hist Avg)'].idxmax()

    # Store optimal metrics & weights
    optimal_portfolios[fund] = {
        'Portfolio': optimal_label,
        'Expected Return': metrics_table.loc['Expected Return', optimal_label],
        'Expected Volatility': metrics_table.loc['Expected Volatility', optimal_label],
        'Sharpe Ratio': metrics_table.loc['Sharpe (Hist Avg)', optimal_label],
        'Avg Yield': metrics_table.loc['Avg Yield', optimal_label],
        'Avg Duration': metrics_table.loc['Avg Duration', optimal_label],
        'Avg Rating': metrics_table.loc['Avg Rating', optimal_label],
        'Weights': weights_table[optimal_label]
    }

# Build Optimal Portfolio Summary Table
optimal_summary = pd.DataFrame({
    fund: {
        'Portfolio': details['Portfolio'],
        'Expected Return': details['Expected Return'],
        'Expected Volatility': details['Expected Volatility'],
        'Sharpe Ratio': details['Sharpe Ratio'],
        'Avg Yield': details['Avg Yield'],
        'Avg Duration': details['Avg Duration'],
        'Avg Rating': details['Avg Rating'],
    }
    for fund, details in optimal_portfolios.items()
})

optimal_summary = optimal_summary.T  # Transpose for readability

"""#Generative AI Assessment and Summary"""

import google.generativeai as genai
from IPython.display import Markdown
from google.colab import userdata

genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))
genai_model = genai.GenerativeModel("gemini-1.5-pro-latest")

def generate_fund_section(fund_name):
    metrics_table = results[fund_name]["Metrics Table"]
    weights_table = results[fund_name]["Weights Table"]
    metrics_summary = metrics_table.to_string()
    weights_summary = weights_table.to_string()

    constraints = fund_constraints[fund_name]
    min_rating_str = inverse_rating_scale[constraints["min_rating"]]

    fund_section = f"""
### 📈 {fund_name} – Efficient Frontier Summary

**Constraints Applied:**
- Max Non-Investment Grade Exposure: {constraints["max_non_ig"]:.0%}
- Max Emerging Market Exposure: {constraints["max_em"]:.0%}
- Max AT1 Exposure: {constraints["max_at1"]:.0%}
- Max Global Hybrid Exposure: {constraints["max_hybrid"]:.0%}
- Max Duration: {constraints['max_duration'] if constraints['max_duration'] else 'None'}
- Min Average Rating: {min_rating_str}

**Portfolio Metrics:**
{metrics_summary}

**Portfolio Weights:**
{weights_summary}
"""
    return fund_section


summary_prompt = f"""
## 🔍 Mean-Variance Optimisation (MVO) Portfolio Review

You are a quantitative fixed income portfolio manager.

Your task is to analyse the historical dataset and the mean-variance optimisation results across three funds (GFI, GCF, EYF) and explain and justify the methods chosen to calculate mean-variance optimisation, including cvxpy. The Funds are constrained through a combination of their respective offering documents and the investment manager's preference to prioritise drawdown avoidance.

You are an excellent data analyst and written communicator and know how to communicate it in well structured, easily readable language suitable for a detail oriented portfolio manager who does not have visibility of the model and wants to be presented with the relevant metrics. Correspondingly, you need to present the findings of the analysis in your response including breakdowns of the optimal portfolio into their respective weights and metrics.

---

### 1️⃣ How The Model Works

- This analysis applies **Mean-Variance Optimisation (MVO)** using Python, leveraging the **cvxpy** package for convex optimisation.
- Covariance matrices of asset returns are estimated using the **Ledoit-Wolf shrinkage estimator** from **scikit-learn**, improving robustness of risk estimation in noisy financial data.
- The model assumes:
  - Historical realised returns and volatilities are reasonable proxies for future expectations.
  - Risk-free rate is calculated from historical returns of US T-Bills.
  - Constraints reflect regulatory, mandate, or investment philosophy limits.
  - The correlation structure of asset returns is stable across the analysis window.

---

### 2️⃣ Overview of The Dataset

- Time Period Covered: {df_common.index.min().date()} to {df_common.index.max().date()}
- Number of Trading Days: {len(df_common)}
- Number of Assets: {len(df_common.columns)}

#### Annualised Return & Volatility (per asset):
{results_df.to_string()}

#### Correlation Matrix:
{correlation_matrix.to_string()}

---

### 3️⃣ General Observations Across Funds

- Are efficient frontiers well-shaped and continuous?
- Do any funds appear especially constrained?
- Are there consistent factors limiting higher returns?
- Are higher returns associated with significant increases in volatility?
- Recommendations for improving feasibility, flexibility, or overall efficiency of portfolios.

---

### 4️⃣ Optimal Portfolios Across Funds

#### Portfolio Characteristics of Optimal Portfolios for Each Fund

| Metric                  | GFI Optimal | GCF Optimal | EYF Optimal |
|------------------------|-------------|-------------|-------------|
{optimal_summary.to_string(index=True)}

---

### 5️⃣ Fund Specific Analysis

{generate_fund_section('GFI')}

{generate_fund_section('GCF')}

{generate_fund_section('EYF')}

---

### 6️⃣ Recommendations

- Suggested improvements to optimisation approach
- Potential additional constraints or relaxations
- Any modelling limitations, assumptions to challenge, or ideas for future refinement
"""

AIFeedback = genai_model.generate_content(summary_prompt)
display(Markdown(AIFeedback.text))